# Food Aggregator Data Pipeline ğŸ½ï¸

## Overview

This project implements a **scalable ELT pipeline** using **Snowflake, dbt, and Dagster**, enabling efficient data ingestion, transformation, and real-time analytics. The pipeline processes structured data, applies incremental transformations, and visualizes insights using **Streamlit**.

## Architecture

![Architecture Diagram](architecture.png)

### **Key Components**

1. **Ingestion Layer** ğŸ“¥

   - Raw data (CSV) is loaded into Snowflake.
   - Uses **COPY INTO** for batch ingestion.

2. **Transformation Layer** ğŸ”„

   - dbt applies transformations to convert raw data into **structured tables**.
   - Implements **incremental models** to process only new/changed records, reducing query costs by **80%**.

3. **Consumption Layer** ğŸ—ï¸

   - Data is stored as **views in Snowflake**, optimized for analytics.
   - Ensures **faster query performance** for dashboards.

4. **Orchestration with Dagster** âš™ï¸

   - Automates pipeline execution & scheduling.
   - Manages dependencies between ingestion, transformation, and analytics.

5. **Analytics & Visualization** ğŸ“Š
   - Processed data is visualized in **Streamlit** dashboards.
   - Provides **real-time and hourly trend analysis** for insights.

---

## ğŸ› ï¸ Technologies Used

- **Snowflake** â€“ Cloud data warehouse for storing raw & processed data.
- **dbt (Data Build Tool)** â€“ Handles **incremental transformations** in Snowflake.
- **Dagster** â€“ Orchestrates ELT workflows, ensuring automation & scheduling.
- **Streamlit** â€“ Builds interactive dashboards for real-time analytics.
- **AWS** â€“ Cloud infrastructure for hosting & data storage.

---

## ğŸ“Œ Features

**End-to-End Automation** â€“ Fully managed ELT pipeline with automated workflows.  
**Incremental Data Processing** â€“ dbt processes only **delta data**, reducing costs.  
**Scalable Architecture** â€“ Can handle **large datasets** with Snowflake.  
**Real-Time Analytics** â€“ Live dashboards powered by **Streamlit & Snowflake Views**.

---

## âš¡ How to Run the Pipeline

### **1ï¸âƒ£ Setup Environment**

Ensure you have **dbt, Dagster, and Streamlit** installed:

```sh
pip install dbt-snowflake dagster dagster-snowflake streamlit

```

### Configure dbt profiles for Snowflake

Set up your **dbt profiles** in `~/.dbt/profiles.yml`:

```yaml
your_profile_name:
  outputs:
    dev:
      type: snowflake
      account: "<your-snowflake-account>"
      user: "<your-username>"
      password: "<your-password>"
      role: "ACCOUNTADMIN"
      database: "<your-database>"
      warehouse: "<your-warehouse>"
      schema: "staging"
```

### **2ï¸âƒ£ Run dbt Transformations**

For a **full refresh**, run:

```sh
dbt run --full-refresh
```

For incremental updates, run:

```sh
dbt run
```

### **3ï¸âƒ£ Orchestrate Pipeline with Dagster**

Start Dagster:

```sh
dagster dev
```

### **4ï¸âƒ£ Start Streamlit Dashboard**

Run the Streamlit dashboard:

```sh
dagster dev
```
